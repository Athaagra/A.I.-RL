def DQN():
    import torch
    import torch.nn as nn
    import torch.optim as optim
    import gym
    import random
    import math
    import time
    import highway_env
    import matplotlib.pyplot as plt

    # if gpu is to be used
    use_cuda = torch.cuda.is_available()
    
    device = torch.device("cuda:0" if use_cuda else "cpu")
    Tensor = torch.Tensor
    LongTensor = torch.LongTensor
    
    env = gym.make("highway-v0")

    seed_value = 0
    env.seed(seed_value)
    torch.manual_seed(seed_value)
    random.seed(seed_value)

    ##### PARAMS ######
    learning_rate = 0.02
    num_episodes = 100
    gamma = 1

    hidden_layer = 320
    
    replay_mem_size = 500
    batch_size = 5

    update_target_frequency = 100

    egreedy = 0.9
    egreedy_final = 0.5
    egreedy_decay = 0.0002

    report_interval = 10
    score_to_solve = 200

    clip_error = False

    ####################

    number_of_inputs = env.observation_space.shape[0]
    number_of_outputs = env.action_space.n
    
    def calculate_epsilon(steps_done):
        epsilon = egreedy_final + (egreedy - egreedy_final)* math.exp(-1. * steps_done / egreedy_decay)
        return epsilon

    class ExperienceReplay(object):
        def __init__(self, capacity):
            self.capacity = capacity
            self.memory = []
            self.position = 0
        
        def push(self, state, action, new_state, reward, done):
            transition = (state, action, new_state, reward, done)
        
            if self.position >= len(self.memory):
                self.memory.append(transition)
            else:
                self.memory[self.position] = transition

            self.position = ( self.position + 1 ) % self.capacity    
        def sample(self, batch_size):
            return zip(*random.sample(self.memory, batch_size))
    
        def __len__(self):
            return len(self.memory)
    
    class NeuralNetwork(nn.Module):
        def __init__(self):
            super(NeuralNetwork, self).__init__()
            self.linear1 = nn.Linear(number_of_inputs,hidden_layer)
            self.linear2 = nn.Linear(hidden_layer,number_of_outputs)
            self.activation = nn.Tanh()
    
        def forward(self, x):
            output1 = self.linear1(x)
            output1 = self.activation(output1)
            output2 = self.linear2(output1)
        
            return output2

    class QNet_Agent(object):
        def __init__(self):
            self.nn = NeuralNetwork().to(device)
            self.target_nn = NeuralNetwork().to(device)
            self.loss_func = nn.MSELoss()
        
            self.optimizer = optim.RMSprop(params=self.nn.parameters(), lr=learning_rate)
        
            self.update_target_counter = 0
        def select_action(self,state,epsilon):
        
            random_for_egreedy = torch.rand(1)[0]
        
            if random_for_egreedy > epsilon:
            
                with torch.no_grad():
                
                    state = Tensor(state).to(device)
                    action_from_nn = self.nn(state)
                    action = torch.max(action_from_nn,0)[1]
                    action = action[0].item()
            else:
                action = env.action_space.sample()
            
            return action
        
        def optimize(self):
        
            if (len(memory) < batch_size):
                return
        
            state, action, new_state, reward, done = memory.sample(batch_size)
        
            state = Tensor(state).to(device)
            new_state = Tensor(new_state).to(device)
            reward = Tensor(reward).to(device)
            action = LongTensor(action).to(device)
            #print("State:{}".format(self.nn(state))
            #print("Squeeze Tensor {}".format(action.unsqueeze(1).squeeze(1)))
            done = Tensor(done).to(device)
        
            new_state_value = self.target_nn(new_state).detach()
            max_new_state_values = torch.max(new_state_value, 1)[0]
            #print(max_new_state_values)
            target_value = reward + ( 1 - done) * gamma * max_new_state_values
            nrows = len(self.nn(state))
            result = torch.zeros((nrows,1), dtype=torch.float32)
            #print(len(result))
            for i in range(nrows):
                #print(i)
                idx = action.unsqueeze(0)
                #print(idx)
                x = self.nn(state)[i][idx]
                #print("X variable:{}".format(x[0][0]))
                #print("result {}".format(result[i][0]))
                result = x
            #print(result)
            predicted_value = result
            #predicted_value = torch.gather(action.squeeze(0), 0, self.nn(state)).squeeze(1)
        
            loss = self.loss_func(predicted_value, target_value)
        
            self.optimizer.zero_grad()
            loss.backward()
        
            if clip_error:
                for param in self.nn.parameters():
                    param.grad.data.clamp_(-1,)
            self.optimizer.step()
        
            if self.update_target_counter % update_target_frequency == 0:
                self.target_nn.load_state_dict(self.nn.state_dict())
        
            self.update_target_counter += 1

    memory = ExperienceReplay(replay_mem_size)
    qnet_agent = QNet_Agent()

    steps_total = []

    frames_total = 0
    solved_after = 0 
    
    for i_episode in range(num_episodes):
        state = env.reset()
        step = 0
        while True:
            step = step + 1
            epsilon = calculate_epsilon(frames_total)
            action = qnet_agent.select_action(state, epsilon)
            new_state, reward, done, info = env.step(action)
            memory.push(state, action, new_state, reward, done)
            qnet_agent.optimize()
            state = new_state
            if done:
                steps_total.append(step)
                print("Episode finished after DQN %i steps" % step)
                break

    print("Average reward: %.2f" % (sum(steps_total)/num_episodes))
    print("Average reward (last 100 episodes): %.2f" % (sum(steps_total[-100:])/100))
    plt.plot(steps_total)
    plt.xlabel('Number of Episodes')
    plt.ylabel('Average Rewards')
    plt.title('Deep Q learning  learning rate= {} e-greedy = {} '.format(learning_rate,egreedy))
    plt.grid(True)
    plt.show()

DQN()
